# Trajectories Directory

This directory contains the execution trajectories and outputs from the browser agent running on WebArena benchmark tasks. Each task's trajectory includes step-by-step information, logs, and screenshots captured during the agent's execution.

## Directory Structure

```
trajectories/
├── results/          # JSON files containing detailed step and summary information
├── logs/             # Text logs of task execution
├── screenshots/      # Screenshots captured at each step
└── README.md         # This file
```

## Contents

### 1. Results (`results/`)

Contains task-level directories with detailed execution information in JSON format.

**Structure:**
```
results/
└── task_{task_id}/
    ├── steps_info_0.json
    ├── steps_info_1.json
    ├── ...
    ├── steps_info_N.json
    └── summary_info.json
```

#### Step Info Files (`steps_info_*.json`)

Each `steps_info_N.json` file contains comprehensive information about a single execution step:

- **step**: Step number (0-indexed)
- **obs**: Observation data from the environment, including chat messages, goal, last action, open pages, accessibility tree (`axtree_txt`), and pruned HTML (`pruned_html`)
- **reward**: Reward received for this step (float value)
- **raw_reward**: Raw reward value before processing (may be null)
- **terminated**: Whether the episode has ended (boolean)
- **truncated**: Whether the episode was truncated due to limits (boolean)
- **action**: The action taken by the agent in this step, including `<think>` reasoning and `<action>` command blocks
- **stats**: Step-level statistics including token counts (`n_token_*`) for various components and timing information (`step_elapsed`, `agent_elapsed`)
- **profiling**: Detailed timing profiling with timestamps for environment steps, action execution, and agent processing phases
- **task_info**: Additional task-specific information (usually empty)

**Example structure:**
```json
{
  "step": 0,
  "obs": {
    "chat_messages": [...],
    "goal": "What is the top-1 best-selling product type in Quarter 1 2022",
    "open_pages_urls": ["http://example.com/admin/dashboard/"],
    "axtree_txt": "RootWebArea 'Dashboard'...",
    "pruned_html": "<head>...</head>..."
  }
}
```

#### Summary Info File (`summary_info.json`)

Contains aggregated statistics and final status for the entire task execution:

- **n_steps**: Total number of steps executed
- **cum_reward**: Cumulative reward received
- **cum_raw_reward**: Raw cumulative reward (before processing)
- **err_msg**: Error message if task failed (null if successful)
- **stack_trace**: Stack trace for errors (null if no error)
- **terminated**: Whether the task completed normally (true/false)
- **truncated**: Whether the task was truncated due to limits (true/false)
- **stats**: Detailed execution statistics
  - `cum_steps`: Cumulative step count
  - `cum_n_token_*`: Token usage statistics for various components
  - `max_n_token_*`: Maximum tokens used in any single step
  - `cum_step_elapsed`: Total elapsed time for all steps (seconds)
  - `max_step_elapsed`: Maximum time for a single step (seconds)
  - `cum_agent_elapsed`: Total agent processing time (seconds)
  - `max_agent_elapsed`: Maximum agent processing time for a single step (seconds)

**Example structure:**
```json
{
  "n_steps": 13,
  "cum_reward": 0.0,
  "err_msg": null,
  "terminated": true,
  "truncated": false,
  "stats": {
    "cum_steps": 14,
    "cum_n_token_goal": 238,
    "cum_step_elapsed": 177.71,
    "cum_agent_elapsed": 1027.27
  }
}
```

### 2. Logs (`logs/`)

Contains timestamped execution logs for each task.

**Structure:**
```
logs/
└── task_{task_id}.log
```

Each log file includes:
- Timestamp and process ID for each log entry
- Experiment configuration and paths
- Step-by-step execution details
- Agent reasoning and action selection (`<think>` and `<action>` blocks)
- HTTP request logs (API calls)
- Progress summaries generated by the agent
- Current state analysis
- Error messages (if any)


### 3. Screenshots (`screenshots/`)

Contains visual snapshots of the browser state at each execution step.

**Structure:**
```
screenshots/
└── task_{task_id}/
    ├── screenshot_som_step_0.png
    ├── screenshot_som_step_1.png
    ├── ...
    └── screenshot_som_step_N.png
```

- **Naming convention**: `screenshot_som_step_{N}.png` where N is the step number (0-indexed)
- **Content**: PNG images showing the browser viewport with Set-of-Marks (SOM) annotations
- **Purpose**: Visual debugging and trajectory analysis

## Task Numbering

Tasks are identified by numeric IDs (e.g., `task_0`, `task_42`, `task_798`). The task IDs correspond to specific benchmark tasks from the WebArena dataset. WebArena is a realistic web environment benchmark for evaluating autonomous agents on complex web-based tasks.

## Usage

### Analyzing a Specific Task

1. **Check the summary**: Read `results/task_{id}/summary_info.json` for overall performance
2. **Review logs**: Open `logs/task_{id}.log` to understand the execution flow
3. **Examine steps**: Inspect individual `steps_info_*.json` files for detailed observations
4. **View screenshots**: Check `screenshots/task_{id}/` for visual confirmation of agent actions

### Calculating Task Success Rate

You can calculate the overall success rate across all tasks:

```python
import json
import os
from pathlib import Path

def calculate_success_rate(results_dir='results'):
    """
    Calculate task success rate based on cum_reward from summary_info.json files.
    
    Success rate calculation:
    - Read cum_reward from each task's summary_info.json
    - If summary_info.json doesn't exist, cum_reward = 0.0
    - Clip cum_reward to maximum of 1.0 (if cum_reward > 1, set to 1)
    - Sum all (clipped) rewards and divide by total number of tasks
    """
    results_path = Path(results_dir)
    task_dirs = [d for d in results_path.iterdir() if d.is_dir() and d.name.startswith('task_')]
    
    total_reward = 0.0
    total_tasks = len(task_dirs)
    
    for task_dir in task_dirs:
        summary_file = task_dir / 'summary_info.json'
        
        if summary_file.exists():
            try:
                with open(summary_file, 'r') as f:
                    summary = json.load(f)
                    cum_reward = summary.get('cum_reward', 0.0)
                    # Clip reward to maximum of 1.0
                    clipped_reward = min(cum_reward, 1.0)
                    total_reward += clipped_reward
            except (json.JSONDecodeError, KeyError):
                # If file is malformed, treat as 0.0
                total_reward += 0.0
        else:
            # If summary file doesn't exist, cum_reward = 0.0
            total_reward += 0.0
    
    success_rate = total_reward / total_tasks if total_tasks > 0 else 0.0
    
    print(f"Total tasks: {total_tasks}")
    print(f"Total reward (clipped): {total_reward:.2f}")
    print(f"Success rate: {success_rate:.2%}")
    
    return success_rate

# Run calculation
success_rate = calculate_success_rate()
```
